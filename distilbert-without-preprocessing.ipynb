{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10023380,"sourceType":"datasetVersion","datasetId":6172410}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:08:43.468535Z","iopub.execute_input":"2024-11-27T03:08:43.469517Z","iopub.status.idle":"2024-11-27T03:08:43.474561Z","shell.execute_reply.started":"2024-11-27T03:08:43.469461Z","shell.execute_reply":"2024-11-27T03:08:43.473582Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\ntrain_df = pd.read_csv('/kaggle/input/dataset/train.csv')\ntest_df = pd.read_csv('/kaggle/input/dataset/test.csv')\nval_df = pd.read_csv('/kaggle/input/dataset/validation.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:08:43.476232Z","iopub.execute_input":"2024-11-27T03:08:43.476557Z","iopub.status.idle":"2024-11-27T03:08:43.560375Z","shell.execute_reply.started":"2024-11-27T03:08:43.476528Z","shell.execute_reply":"2024-11-27T03:08:43.559535Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nimport spacy\n\nif not spacy.util.is_package('en_core_web_sm'):\n    spacy.cli.download(\"en_core_web_sm\")\n\nnlp = spacy.load('en_core_web_sm')\n\n\ndef preprocess_text(text):\n    \"\"\" Method to clean reviews from noise and standardize text across the different classes.\n    Arguments\n    ---------\n    text : String\n        Text to clean\n    Returns\n    -------\n    text : String\n        Cleaned text\n    \"\"\"\n    stop_words = set(stopwords.words('english'))\n    \n    text = text.lower() # Convert text to lowercase\n    text = text.replace('\\n', ' ') # Remove newline characters and extra spaces\n    text = re.sub(r'http\\S+', '', text) # Remove URLs\n    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation and special characters\n    text = re.sub(r'\\d+', ' ', text) # Remove digits\n    text = ' '.join([word for word in text.split() if word not in stop_words]) # Remove stopwords\n    text = ' '.join( [token.lemma_ for token in nlp(text)]) # Lemmatize text\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:08:43.561441Z","iopub.execute_input":"2024-11-27T03:08:43.561790Z","iopub.status.idle":"2024-11-27T03:08:44.243470Z","shell.execute_reply.started":"2024-11-27T03:08:43.561732Z","shell.execute_reply":"2024-11-27T03:08:44.242356Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# train_df['text'] = train_df['text'].apply(lambda x: preprocess_text(x))\n# val_df['text'] = val_df['text'].apply(lambda x: preprocess_text(x))\n# test_df['text'] = test_df['text'].apply(lambda x: preprocess_text(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:08:44.245075Z","iopub.execute_input":"2024-11-27T03:08:44.245506Z","iopub.status.idle":"2024-11-27T03:08:44.250605Z","shell.execute_reply.started":"2024-11-27T03:08:44.245459Z","shell.execute_reply":"2024-11-27T03:08:44.249396Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_df['label'] = train_df['account.type'].apply(lambda x: 1 if x == 'human' else 0)\ntest_df['label'] = test_df['account.type'].apply(lambda x: 1 if x == 'human' else 0)\nval_df['label'] = val_df['account.type'].apply(lambda x: 1 if x == 'human' else 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:08:44.252643Z","iopub.execute_input":"2024-11-27T03:08:44.252955Z","iopub.status.idle":"2024-11-27T03:08:44.276205Z","shell.execute_reply.started":"2024-11-27T03:08:44.252926Z","shell.execute_reply":"2024-11-27T03:08:44.275169Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Select model checkpoint\nmodel_checkpoint = \"distilbert-base-uncased\"  # For ALBERT, use 'albert-base-v2'\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\n# Convert Pandas DataFrames to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Tokenize datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch usage\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:08:44.277616Z","iopub.execute_input":"2024-11-27T03:08:44.278033Z","iopub.status.idle":"2024-11-27T03:08:47.082207Z","shell.execute_reply.started":"2024-11-27T03:08:44.277990Z","shell.execute_reply":"2024-11-27T03:08:47.081300Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20712 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b653252157044cafb6025f03cd24c93f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2558 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac13026ed9541edb56ee72fe8593230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2302 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a0942a699ed4ba1a02d43da5f05e8dd"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:08:47.083338Z","iopub.execute_input":"2024-11-27T03:08:47.083635Z","iopub.status.idle":"2024-11-27T03:08:47.245465Z","shell.execute_reply.started":"2024-11-27T03:08:47.083606Z","shell.execute_reply":"2024-11-27T03:08:47.244499Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    report_to=\"none\",  \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:11:58.084073Z","iopub.execute_input":"2024-11-27T03:11:58.084471Z","iopub.status.idle":"2024-11-27T03:11:58.121809Z","shell.execute_reply.started":"2024-11-27T03:11:58.084435Z","shell.execute_reply":"2024-11-27T03:11:58.120822Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_score(labels, predictions)\n    return {\"accuracy\": accuracy}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:12:01.020808Z","iopub.execute_input":"2024-11-27T03:12:01.021312Z","iopub.status.idle":"2024-11-27T03:12:01.026708Z","shell.execute_reply.started":"2024-11-27T03:12:01.021262Z","shell.execute_reply":"2024-11-27T03:12:01.025694Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:12:01.721185Z","iopub.execute_input":"2024-11-27T03:12:01.721996Z","iopub.status.idle":"2024-11-27T03:12:01.734586Z","shell.execute_reply.started":"2024-11-27T03:12:01.721958Z","shell.execute_reply":"2024-11-27T03:12:01.733570Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:12:02.558576Z","iopub.execute_input":"2024-11-27T03:12:02.558932Z","iopub.status.idle":"2024-11-27T03:38:53.991769Z","shell.execute_reply.started":"2024-11-27T03:12:02.558900Z","shell.execute_reply":"2024-11-27T03:38:53.990773Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6480' max='6480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6480/6480 26:50, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.313500</td>\n      <td>0.271554</td>\n      <td>0.875760</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.131800</td>\n      <td>0.314657</td>\n      <td>0.882276</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.154000</td>\n      <td>0.441174</td>\n      <td>0.875760</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.121200</td>\n      <td>0.589180</td>\n      <td>0.887055</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.001800</td>\n      <td>0.737664</td>\n      <td>0.885317</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.034000</td>\n      <td>0.811598</td>\n      <td>0.878801</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.005800</td>\n      <td>0.829777</td>\n      <td>0.878367</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000800</td>\n      <td>0.899255</td>\n      <td>0.879235</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.005300</td>\n      <td>0.945939</td>\n      <td>0.878367</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000500</td>\n      <td>0.939164</td>\n      <td>0.880104</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6480, training_loss=0.05699822525555332, metrics={'train_runtime': 1610.9592, 'train_samples_per_second': 128.569, 'train_steps_per_second': 4.022, 'total_flos': 6859161902407680.0, 'train_loss': 0.05699822525555332, 'epoch': 10.0})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"results = trainer.evaluate(test_dataset)\nprint(results)\n\n# Generate classification report\npredictions = trainer.predict(test_dataset)\ny_preds = np.argmax(predictions.predictions, axis=-1)\ny_true = test_df['label'].values\nprint(classification_report(y_true, y_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:39:07.327898Z","iopub.execute_input":"2024-11-27T03:39:07.328348Z","iopub.status.idle":"2024-11-27T03:39:20.285084Z","shell.execute_reply.started":"2024-11-27T03:39:07.328298Z","shell.execute_reply":"2024-11-27T03:39:20.284242Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.6083710193634033, 'eval_accuracy': 0.8788115715402658, 'eval_runtime': 6.444, 'eval_samples_per_second': 396.958, 'eval_steps_per_second': 12.415, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.86      0.90      0.88      1280\n           1       0.90      0.85      0.88      1278\n\n    accuracy                           0.88      2558\n   macro avg       0.88      0.88      0.88      2558\nweighted avg       0.88      0.88      0.88      2558\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"model.save_pretrained(\"./saved_model\")\ntokenizer.save_pretrained(\"./saved_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:39:20.287009Z","iopub.execute_input":"2024-11-27T03:39:20.287333Z","iopub.status.idle":"2024-11-27T03:39:21.321469Z","shell.execute_reply.started":"2024-11-27T03:39:20.287301Z","shell.execute_reply":"2024-11-27T03:39:21.320572Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"('./saved_model/tokenizer_config.json',\n './saved_model/special_tokens_map.json',\n './saved_model/vocab.txt',\n './saved_model/added_tokens.json',\n './saved_model/tokenizer.json')"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}